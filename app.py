# app.py
import streamlit as st
import pandas as pd
import numpy as np
import io
import zipfile
import json
import warnings

from sklearn.metrics import silhouette_score
from sklearn.preprocessing import StandardScaler

from utils import load_and_prepare, median_nn_distance
from eda import run_eda
from features import compute_geometric_features, find_local_minima
from clustering import run_kmeans, run_dbscan, run_agglomerative, run_gmm, choose_param_by_silhouette
from visualization import grid_surface, plot_heatmap_matplotlib, plot_3d_plotly, plot_quiver, plot_watershed_matplotlib
from report import build_pdf_report







st.set_page_config(layout="wide", page_title="–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–æ—á–µ–∫ –Ω–∞ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –∑–µ–º–ª–∏")
st.title("–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Ç–æ—á–µ–∫ –Ω–∞ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏ –∑–µ–º–ª–∏")













#
# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
#
uploaded = st.file_uploader("–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV (X,Y,Z)", type=["csv"])
if not uploaded:
    st.info("–ó–∞–≥—Ä—É–∑–∏—Ç–µ CSV —Å–æ —Å—Ç–æ–ª–±—Ü–∞–º–∏ X,Y,Z (–±–µ–∑ —É—á—ë—Ç–∞ —Ä–µ–≥–∏—Å—Ç—Ä–∞).")
    st.stop()

try:
    df = load_and_prepare(uploaded)
except Exception as e:
    st.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ: {e}")
    st.stop()

#
# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å: –≥–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–∞—Å—à—Ç–∞–±–∞ (–Ω–µ –∑–∞–ø—É—Å–∫–∞—é—Ç —Ç—è–∂—ë–ª—ã–µ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å–∞–º–∏ –ø–æ —Å–µ–±–µ)
#

st.sidebar.header("–ú–∞—Å—à—Ç–∞–± –∏ –ø—Ä–∏–∑–Ω–∞–∫–∏")
median_nn = median_nn_distance(df)
st.sidebar.write("–ú–µ–¥–∏–∞–Ω–∞ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏—è –¥–æ –±–ª–∏–∂–∞–π—à–µ–≥–æ —Å–æ—Å–µ–¥–∞:", round(median_nn, 6))

R_factor = st.sidebar.number_input("–ú–Ω–æ–∂–∏—Ç–µ–ª—å R (–ª–æ–∫–∞–ª—å–Ω–æ–µ —Å–æ—Å–µ–¥—Å—Ç–≤–æ)", value=3.0, step=0.5)
R = float(R_factor * median_nn)

R_basin_factor = st.sidebar.number_input("–ú–Ω–æ–∂–∏—Ç–µ–ª—å R_basin", value=2.0, step=0.5)
R_basin = float(R_basin_factor * median_nn)

min_neighbors = int(st.sidebar.number_input("–ú–∏–Ω–∏–º—É–º —Å–æ—Å–µ–¥–µ–π (—Ä–µ–∑–µ—Ä–≤)", value=8, min_value=3))
basin_frac = float(st.sidebar.slider("–î–æ–ª—è –≥–ª—É–±–∏–Ω—ã –¥–ª—è –±–∞—Å—Å–µ–π–Ω–∞ (thr = z + depth * frac)", 0.0, 1.0, 0.5))

st.sidebar.markdown("---")

#
# –ö–Ω–æ–ø–∫–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Ç—è–∂—ë–ª–∞—è –æ–ø–µ—Ä–∞—Ü–∏—è). df_geo —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è –≤ session_state
#
if "df_geo" not in st.session_state:
    st.session_state.df_geo = None

compute = st.sidebar.button("–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –º–∏–Ω–∏–º—É–º—ã")
if compute:
    with st.spinner("–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–∏—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤..."):
        try:
            geo = compute_geometric_features(df, R=R, R_basin=R_basin, min_neighbors=min_neighbors,
                                             basin_frac_of_depth=basin_frac)
            df_geo = pd.concat([df.reset_index(drop=True), geo.reset_index(drop=True)], axis=1)
            df_geo['IS_LOCAL_MIN'] = find_local_minima(df_geo, radius=R)
            st.session_state.df_geo = df_geo
            st.success("–ü—Ä–∏–∑–Ω–∞–∫–∏ —Ä–∞—Å—Å—á–∏—Ç–∞–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Å–µ—Å—Å–∏–∏.")
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {e}")
            st.stop()

# –ï—Å–ª–∏ —Ä–∞–Ω–µ–µ —É–∂–µ —Å—á–∏—Ç–∞–ª–∏ ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º
if st.session_state.df_geo is None:
    st.info("–ù–∞–∂–º–∏—Ç–µ –≤ —Å–∞–π–¥–±–∞—Ä–µ ‚Äò–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –º–∏–Ω–∏–º—É–º—ã‚Äô, —á—Ç–æ–±—ã –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –¥–∞–Ω–Ω—ã–µ.")
    st.stop()

df_geo = st.session_state.df_geo

#
# –¢–∞–±–ª–∏—Ü–∞ –∏ —á–∏—Å–ª–æ –º–∏–Ω–∏–º—É–º–æ–≤
#
st.subheader("–í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ + –≤—ã—á–∏—Å–ª–µ–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–ø–µ—Ä–≤—ã–µ —Å—Ç—Ä–æ–∫–∏)")
st.dataframe(df_geo.head())
n_min = int(df_geo['IS_LOCAL_MIN'].sum())
st.write("–ù–∞–π–¥–µ–Ω–æ –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤:", n_min)
if n_min == 0:
    st.warning("–ú–∏–Ω–∏–º—É–º—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –£–≤–µ–ª–∏—á—å—Ç–µ –º–Ω–æ–∂–∏—Ç–µ–ª—å R –∏–ª–∏ –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –≤—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ.")
    st.stop()

# Prepare minima dataframe for clustering/visualization
mins = df_geo[df_geo['IS_LOCAL_MIN']].reset_index(drop=True)
# ensure orientation-based sin/cos features exist
if 'orientation' in mins.columns:
    mins['ori_sin'] = np.sin(mins['orientation'])
    mins['ori_cos'] = np.cos(mins['orientation'])
else:
    mins['ori_sin'] = 0.0
    mins['ori_cos'] = 0.0

#
# Clustering UI (controls). We will not recompute heavy features here.
#
st.sidebar.header("–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è")
method = st.sidebar.selectbox("–ê–ª–≥–æ—Ä–∏—Ç–º", ["KMeans", "DBSCAN", "Agglomerative", "GMM"])

max_k = min(8, max(2, len(mins)))
default_k = min(3, max_k)
if method in ["KMeans", "Agglomerative", "GMM"]:
    k = st.sidebar.slider("–ß–∏—Å–ª–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ / –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ (k)", min_value=2, max_value=max_k, value=default_k)
else:
    eps = st.sidebar.slider("DBSCAN eps", 0.01, 5.0, 0.7, step=0.01)
    min_samp = st.sidebar.slider("DBSCAN min_samples", 2, 30, 4)

default_feats = ['depth', 'basin_area_est', 'mean_grad', 'mean_curvature', 'gauss_curvature', 'ori_sin', 'ori_cos']
available_feats = [f for f in default_feats if f in mins.columns]
if not available_feats:
    st.error("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —à–∞–≥ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.")
    st.stop()

feats = st.sidebar.multiselect("–ü—Ä–∏–∑–Ω–∞–∫–∏ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏", options=available_feats,
                               default=available_feats[:min(5, len(available_feats))])
if not feats:
    st.error("–í—ã–±–µ—Ä–∏—Ç–µ —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –ø—Ä–∏–∑–Ω–∞–∫ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏.")
    st.stop()

# ---
#
# –∏ –≤—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ –ø–æ silhouette score. –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª–∏–¥–µ—Ä–±–æ—Ä–¥ –≤ session_state.
# ---
st.sidebar.markdown("---")
st.sidebar.subheader("–ê–≤—Ç–æ–ø–æ–¥–±–æ—Ä")
auto_tune = st.sidebar.checkbox("–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ–¥–æ–±—Ä–∞—Ç—å –ª—É—á—à–∏–π –º–µ—Ç–æ–¥ (—Å–∏–ª—É—ç—Ç)", value=False)

if auto_tune:
    k_max_auto = st.sidebar.slider("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ k –¥–ª—è KMeans/Agglomerative/GMM", min_value=2, max_value=max_k, value=default_k)
    eps_min = st.sidebar.number_input("DBSCAN eps min", value=0.2, min_value=0.01, max_value=10.0, step=0.01)
    eps_max = st.sidebar.number_input("DBSCAN eps max", value=1.2, min_value=0.05, max_value=10.0, step=0.05)
    eps_step = st.sidebar.number_input("DBSCAN eps —à–∞–≥", value=0.1, min_value=0.01, max_value=2.0, step=0.01)
    min_samp_auto = st.sidebar.slider("DBSCAN min_samples (–∞–≤—Ç–æ)", 2, 30, 4)

    auto_btn = st.sidebar.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –∞–≤—Ç–æ–ø–æ–¥–±–æ—Ä")
    if auto_btn:
        with st.spinner("–ü–æ–¥–±–∏—Ä–∞—é –∞–ª–≥–æ—Ä–∏—Ç–º –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–æ —Å–∏–ª—É—ç—Ç—É..."):
            try:
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è
                X = mins[feats].fillna(0).values
                scaler = StandardScaler()
                Xs = scaler.fit_transform(X)

                candidates = []

                # –ü–µ—Ä–µ–±–æ—Ä k –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–µ—Ç–æ–¥–æ–≤
                k_values = list(range(2, int(k_max_auto) + 1))
                res_km = choose_param_by_silhouette(Xs, run_kmeans, 'n_clusters', k_values)
                candidates.append(("KMeans", 'k', res_km))

                res_ag = choose_param_by_silhouette(Xs, run_agglomerative, 'n_clusters', k_values)
                candidates.append(("Agglomerative", 'k', res_ag))

                res_gm = choose_param_by_silhouette(Xs, run_gmm, 'n_components', k_values)
                candidates.append(("GMM", 'k', res_gm))

                 # –ü–µ—Ä–µ–±–æ—Ä eps –¥–ª—è DBSCAN (min_samples —Ñ–∏–∫—Å–∏—Ä—É–µ–º)
                eps_values = np.arange(float(eps_min), float(eps_max) + 1e-12, float(eps_step)).tolist()
                res_db = choose_param_by_silhouette(
                    Xs,
                    lambda X, **p: run_dbscan(X, **p),
                    'eps',
                    eps_values,
                    fixed_params={'min_samples': int(min_samp_auto)}
                )
                candidates.append(("DBSCAN", 'eps', res_db))

                # –í—ã–±–æ—Ä –ª—É—á—à–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
                leaderboard = []
                best = None
                best_algo = None
                best_param_name = None
                for name, pname, resobj in candidates:
                    import numpy as np
                    score = resobj.best_score
                    leaderboard.append({
                        'algo': name,
                        'param': pname,
                        'value': resobj.best_param,
                        'silhouette': None if np.isneginf(score) else float(score)
                    })
                    if best is None or score > best.best_score:
                        best = resobj
                        best_algo = name
                        best_param_name = pname

                # –ü—Ä–∏–º–µ–Ω—è–µ–º –ª—É—á—à–∏–π
                labels = np.asarray(best.best_labels)
                mins['cluster'] = labels
                st.session_state.cluster_result = {
                    "labels": labels,
                    "model": best.best_model,
                    "feats": feats,
                    "Xs": Xs,
                    "algo": best_algo,
                    "param": {best_param_name: best.best_param}
                }
                st.session_state.auto_leaderboard = leaderboard

                best_sil = None if np.isneginf(best.best_score) else round(float(best.best_score), 4)
                st.success(f"–õ—É—á—à–∏–π –º–µ—Ç–æ–¥: {best_algo} ({best_param_name}={best.best_param}), silhouette={best_sil}")
            except Exception as e:
                st.error(f"–ê–≤—Ç–æ–ø–æ–¥–±–æ—Ä –Ω–µ —É–¥–∞–ª—Å—è: {e}")

# ---
# –ö–Ω–æ–ø–∫–∞ –æ–±—ã—á–Ω–æ–≥–æ –∑–∞–ø—É—Å–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ (–±–µ–∑ –∞–≤—Ç–æ–ø–æ–¥–±–æ—Ä–∞)
# ---
if "cluster_result" not in st.session_state:
    st.session_state.cluster_result = None

cluster_run = st.sidebar.button("–ó–∞–ø—É—Å—Ç–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é")
if cluster_run:
    with st.spinner("–í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è..."):
        try:
            X = mins[feats].fillna(0).values
            scaler = StandardScaler()
            Xs = scaler.fit_transform(X)

            if method == "KMeans":
                labels, model = run_kmeans(Xs, n_clusters=k)
            elif method == "Agglomerative":
                labels, model = run_agglomerative(Xs, n_clusters=k)
            elif method == "GMM":
                labels, model = run_gmm(Xs, n_components=k)
            else:
                labels, model = run_dbscan(Xs, eps=eps, min_samples=min_samp)

            labels = np.asarray(labels)
            mins['cluster'] = labels
            st.session_state.cluster_result = {"labels": labels, "model": model, "feats": feats, "Xs": Xs}
            st.success("–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ —Å–µ—Å—Å–∏–∏.")
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            st.session_state.cluster_result = None

# –ï—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏ –µ—â—ë –Ω–µ—Ç ‚Äî –ø—Ä–µ–¥–ª–æ–∂–∏–º –∑–∞–ø—É—Å—Ç–∏—Ç—å
if st.session_state.cluster_result is None:
    st.info("–ù–∞–∂–º–∏—Ç–µ ‚Äò–ó–∞–ø—É—Å—Ç–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é‚Äô, —á—Ç–æ–±—ã –≤—ã—á–∏—Å–ª–∏—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ã —Å —Ç–µ–∫—É—â–∏–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.")
    st.stop()

res = st.session_state.cluster_result
labels = res["labels"]
mins['cluster'] = labels

st.subheader("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏")
st.write("–†–∞–∑–º–µ—Ä—ã –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:")
st.write(pd.Series(labels).value_counts())

# Silhouette (handle DBSCAN noise -1)
sil = None
try:
    unique_labels = set(labels.tolist())
    if len(unique_labels) > 1:
        if -1 in unique_labels:
            mask = (labels != -1)
            if mask.sum() >= 2 and len(set(labels[mask].tolist())) > 1:
                sil = silhouette_score(res["Xs"][mask], labels[mask])
        else:
            sil = silhouette_score(res["Xs"], labels)
except Exception:
    sil = None
st.write("–°–∏–ª—É—ç—Ç (silhouette):", sil)

if 'auto_leaderboard' in st.session_state and st.session_state.auto_leaderboard:
    st.write("–õ–∏–¥–µ—Ä–±–æ—Ä–¥ –∞–≤—Ç–æ–ø–æ–¥–±–æ—Ä–∞ (–≤—ã—à–µ ‚Äî –ª—É—á—à–µ):")
    st.dataframe(pd.DataFrame(st.session_state.auto_leaderboard).sort_values('silhouette', ascending=False), use_container_width=True)

# –ê–≤—Ç–æ‚Äë–æ–ø–∏—Å–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (—É–ª—É—á—à–µ–Ω–Ω–æ–µ)
def _categorize(value, low, high):
    if value <= low:
        return "–Ω–∏–∑–∫–∏–π"
    if value >= high:
        return "–≤—ã—Å–æ–∫–∏–π"
    return "—Å—Ä–µ–¥–Ω–∏–π"

def _orientation_to_cardinal(angle_rad: float) -> str:
    # angle in degrees, map to 8-wind rose
    deg = (np.degrees(angle_rad) + 360.0) % 360.0
    labels = [
        "–í–æ—Å—Ç–æ–∫", "–°‚Äë–í", "–°–µ–≤–µ—Ä", "–°‚Äë–ó", "–ó–∞–ø–∞–¥", "–Æ‚Äë–ó", "–Æ–≥", "–Æ‚Äë–í"
    ]
    idx = int((deg + 22.5) // 45) % 8
    return labels[idx]

try:
    agg = {
        'cluster': 'size'
    }
    profile = mins.groupby('cluster').agg(
        count=('cluster', 'size'),
        depth_mean=('depth', 'mean'), depth_min=('depth', 'min'), depth_max=('depth', 'max'),
        basin_area_mean=('basin_area_est', 'mean'), basin_area_std=('basin_area_est', 'std'),
        grad_mean=('mean_grad', 'mean'), grad_median=('mean_grad', 'median'),
        H_mean=('mean_curvature', 'mean'), K_mean=('gauss_curvature', 'mean'),
        ori_sin_mean=('ori_sin', 'mean'), ori_cos_mean=('ori_cos', 'mean')
    ).reset_index()
    profile['orientation'] = np.arctan2(profile['ori_sin_mean'], profile['ori_cos_mean'])

    for col in ['depth_mean', 'basin_area_mean', 'grad_mean', 'H_mean', 'K_mean']:
        lo = profile[col].quantile(0.33)
        hi = profile[col].quantile(0.67)
        profile[f'{col}_cat'] = profile[col].apply(lambda v, lo=lo, hi=hi: _categorize(v, lo, hi))

    def _name_cluster(r):
        if r['depth_mean_cat'] == '–≤—ã—Å–æ–∫–∏–π' and r['basin_area_mean_cat'] == '–≤—ã—Å–æ–∫–∏–π':
            return "–≥–ª—É–±–æ–∫–∏–µ —à–∏—Ä–æ–∫–∏–µ –≤–ø–∞–¥–∏–Ω—ã"
        if r['depth_mean_cat'] == '–≤—ã—Å–æ–∫–∏–π' and r['grad_mean_cat'] == '–≤—ã—Å–æ–∫–∏–π':
            return "–≥–ª—É–±–æ–∫–∏–µ –∫—Ä—É—Ç—ã–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ –≤–ø–∞–¥–∏–Ω—ã"
        if r['depth_mean_cat'] == '–Ω–∏–∑–∫–∏–π' and r['basin_area_mean_cat'] == '–≤—ã—Å–æ–∫–∏–π':
            return "–ø–æ–ª–æ–≥–∏–µ —à–∏—Ä–æ–∫–∏–µ —á–∞—à–∏"
        if r['grad_mean_cat'] == '–≤—ã—Å–æ–∫–∏–π':
            return "–∫—Ä—É—Ç—ã–µ –ª–æ–∫–∞–ª—å–Ω—ã–µ —Ñ–æ—Ä–º—ã"
        return "—É–º–µ—Ä–µ–Ω–Ω—ã–µ –≤–ø–∞–¥–∏–Ω—ã"

    def _describe_row(r):
        parts = []
        parts.append(f"—Ç–∏–ø: {_name_cluster(r)}")
        parts.append(f"—Ä–∞–∑–º–µ—Ä: n={int(r['count'])}")
        parts.append(f"–≥–ª—É–±–∏–Ω–∞: {r['depth_mean_cat']} (—Å—Ä={r['depth_mean']:.3f}, –º–∏–Ω={r['depth_min']:.3f}, –º–∞–∫—Å={r['depth_max']:.3f})")
        parts.append(f"–ø–ª–æ—â–∞–¥—å –±–∞—Å—Å–µ–π–Ω–∞: {r['basin_area_mean_cat']} (—Å—Ä={r['basin_area_mean']:.3f}, œÉ={0.0 if np.isnan(r['basin_area_std']) else float(r['basin_area_std']):.3f})")
        parts.append(f"–≥—Ä–∞–¥–∏–µ–Ω—Ç: {r['grad_mean_cat']} (—Å—Ä={r['grad_mean']:.3f}, –º–µ–¥={r['grad_median']:.3f})")
        parts.append(f"–∫—Ä–∏–≤–∏–∑–Ω—ã: H={r['H_mean_cat']} (—Å—Ä={r['H_mean']:.3f}); K={r['K_mean_cat']} (—Å—Ä={r['K_mean']:.3f})")
        parts.append(f"–æ—Ä–∏–µ–Ω—Ç–∞—Ü–∏—è: {_orientation_to_cardinal(r['orientation'])} (‚âà{np.degrees(r['orientation']):.0f}¬∞)")
        return "; ".join(parts)

    profile['name'] = profile.apply(_name_cluster, axis=1)
    profile['description'] = profile.apply(_describe_row, axis=1)
    st.subheader("–û–ø–∏—Å–∞–Ω–∏–µ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤")
    st.dataframe(profile[['cluster', 'name', 'count', 'depth_mean', 'basin_area_mean', 'grad_mean', 'H_mean', 'K_mean', 'description']], use_container_width=True)
    # –≠–∫—Å–ø–æ—Ä—Ç –æ–ø–∏—Å–∞–Ω–∏–π
    desc_json = profile[['cluster', 'name', 'description']].to_json(orient='records', force_ascii=False).encode('utf-8')
    st.download_button("–°–∫–∞—á–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (JSON)", data=desc_json, file_name="cluster_descriptions.json", mime="application/json")
    desc_csv = profile[['cluster', 'name', 'description']].to_csv(index=False).encode('utf-8')
    st.download_button("–°–∫–∞—á–∞—Ç—å –æ–ø–∏—Å–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (CSV)", data=desc_csv, file_name="cluster_descriptions.csv", mime="text/csv")
except Exception:
    pass

# –°–ø—Ä–∞–≤–∫–∞ –ø–æ –ø—Ä–∏–∑–Ω–∞–∫–∞–º –∏ —Ñ–æ—Ä–º—É–ª–∞–º (—Å—Ç–∞–±–∏–ª—å–Ω—ã–π —Ä–µ–Ω–¥–µ—Ä LaTeX)
with st.expander("–°–ø—Ä–∞–≤–∫–∞: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏ —Ñ–æ—Ä–º—É–ª—ã"):
    st.write("depth:")
    st.latex(r"\mathrm{depth}_i = \max\bigl(0,\, \overline{z_{\mathcal{N}(i)}} - z_i\bigr)")
    st.write("–ü–æ—Ä–æ–≥ –±–∞—Å—Å–µ–π–Ω–∞:")
    st.latex(r"\mathrm{thr}_i = z_i + \mathrm{depth}_i\cdot \mathrm{frac}")
    st.write("–í –ø–ª–æ—â–∞–¥—å –ø–æ–ø–∞–¥–∞—é—Ç —Å–æ—Å–µ–¥–∏ –ø—Ä–∏ —É—Å–ª–æ–≤–∏–∏:")
    st.latex(r"z_j \le \mathrm{thr}_i,\; j\in \mathcal{N}_{R_{\mathrm{basin}}}(i)")
    st.write("–û—Ü–µ–Ω–∫–∞ –ø–ª–æ—â–∞–¥–∏ –±–∞—Å—Å–µ–π–Ω–∞:")
    st.latex(r"\widehat{A}_i = \pi R_{\mathrm{basin}}^2\, \cdot \, \frac{\bigl|\{j\in \mathcal{N}_{R_{\mathrm{basin}}}(i): z_j \le \mathrm{thr}_i\}\bigr|}{\bigl|\mathcal{N}_{R_{\mathrm{basin}}}(i)\bigr|}")
    st.write("–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏:")
    st.latex(r"z = A x^2 + B y^2 + C x y + D x + E y + F")
    st.write("–ì—Ä–∞–¥–∏–µ–Ω—Ç –∏ –µ–≥–æ –Ω–æ—Ä–º–∞:")
    st.latex(r"\nabla z = (\partial z/\partial x,\, \partial z/\partial y) = (D, E),\quad \mathrm{mean\_grad}=\sqrt{D^2+E^2}")
    st.write("–ö—Ä–∏–≤–∏–∑–Ω—ã (–ø–æ –∫–≤–∞–¥—Ä–∞—Ç–∏—á–Ω–æ–π –º–æ–¥–µ–ª–∏):")
    st.latex(r"H = \dfrac{(1+E^2)\,2A - 2DE\,C + (1+D^2)\,2B}{2\,(1+D^2+E^2)^{3/2}}")
    st.latex(r"K = \dfrac{(2A)(2B) - C^2}{(1+D^2+E^2)^2}")
    st.write("–û—Ä–∏–µ–Ω—Ç–∞—Ü–∏—è:")
    st.latex(r"\theta = \mathrm{atan2}(E, D)")
    st.write("–ö—Ä–∏—Ç–µ—Ä–∏–π –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –º–∏–Ω–∏–º—É–º–∞:")
    st.latex(r"z_i < z_j\;\; \forall\, j\in\mathcal{N}_R(i) ")

#
# Visualizations + optional watershed
#
XI, YI, ZI = grid_surface(df_geo, grid_res=200)

do_watershed = st.sidebar.checkbox("–†–∞—Å—Å—á–∏—Ç–∞—Ç—å –±–∞—Å—Å–µ–π–Ω—ã –º–µ—Ç–æ–¥–æ–º –≤–æ–¥–æ—Ä–∞–∑–¥–µ–ª–∞ (—Ç–æ—á–Ω–∞—è –ø–ª–æ—â–∞–¥—å)", value=False)
basin_areas = None

if do_watershed:
    try:
        from skimage.segmentation import watershed
        from scipy import ndimage as ndi
        import numpy as np

        # –î–µ–ª–∞–µ–º —Ä–∞–±–æ—á—É—é –∫–æ–ø–∏—é ZI, —á—Ç–æ–±—ã –Ω–µ –∏—Å–ø–æ—Ä—Ç–∏—Ç—å –∏—Å—Ö–æ–¥–Ω—ã–π –º–∞—Å—Å–∏–≤
        ZI_work = ZI.copy()

        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –±–ª–∏–∂–∞–π—à–∏–º–∏ –≤–∞–ª–∏–¥–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ —á–µ—Ä–µ–∑ distance transform
        nan_mask = np.isnan(ZI_work)
        if nan_mask.any():
            # –ò–Ω–¥–µ–∫—Å—ã –±–ª–∏–∂–∞–π—à–∏—Ö –≤–∞–ª–∏–¥–Ω—ã—Ö –ø–∏–∫—Å–µ–ª–µ–π
            indices = ndi.distance_transform_edt(
                nan_mask,
                return_distances=False,
                return_indices=True
            )
            # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ –±–ª–∏–∂–∞–π—à–∏—Ö –≤–∞–ª–∏–¥–Ω—ã—Ö —Å–æ—Å–µ–¥–µ–π
            ZI_work[nan_mask] = ZI_work[tuple(indices[:, nan_mask])]

        # –ò–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å, —á—Ç–æ–±—ã –º–∏–Ω–∏–º—É–º—ã —Å—Ç–∞–ª–∏ "–ø–∏–∫–∞–º–∏" –¥–ª—è –≤–æ–¥–æ—Ä–∞–∑–¥–µ–ª–∞
        inv = np.max(ZI_work) - ZI_work

        # –ö–æ–æ—Ä–¥–∏–Ω–∞—Ç–Ω—ã–µ —Å–µ—Ç–∫–∏
        x_min, x_max = df_geo['X'].min(), df_geo['X'].max()
        y_min, y_max = df_geo['Y'].min(), df_geo['Y'].max()
        xi = np.linspace(x_min, x_max, ZI_work.shape[1])
        yi = np.linspace(y_min, y_max, ZI_work.shape[0])

        # –ú–∞—Ä–∫–µ—Ä—ã –≤ –ø–æ–∑–∏—Ü–∏—è—Ö –ª–æ–∫–∞–ª—å–Ω—ã—Ö –º–∏–Ω–∏–º—É–º–æ–≤
        marker_mask = np.zeros_like(ZI_work, dtype=bool)
        for _, r in mins.iterrows():
            # –ë–ª–∏–∂–∞–π—à–∏–µ –∏–Ω–¥–µ–∫—Å—ã —Å–µ—Ç–∫–∏
            ix = int(np.searchsorted(xi, r['X']))
            iy = int(np.searchsorted(yi, r['Y']))

            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏–Ω–¥–µ–∫—Å—ã –≤–∞–ª–∏–¥–Ω—ã–º–∏ –≥—Ä–∞–Ω–∏—Ü–∞–º–∏
            ix = np.clip(ix, 0, ZI_work.shape[1] - 1)
            iy = np.clip(iy, 0, ZI_work.shape[0] - 1)

            marker_mask[iy, ix] = True

        # –ù—É–º–µ—Ä—É–µ–º —Å–≤—è–∑–Ω—ã–µ –º–∞—Ä–∫–µ—Ä—ã (–Ω–∞ —Å–ª—É—á–∞–π –¥—É–±–ª–∏–∫–∞—Ç–æ–≤)
        markers, num_markers = ndi.label(marker_mask.astype(int))
        if num_markers == 0:
            raise ValueError("No valid markers found for watershed.")

        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤–æ–¥–æ—Ä–∞–∑–¥–µ–ª
        labels_ws = watershed(inv, markers=markers, mask=~nan_mask)  # Respect original NaN regions

        # –ü–ª–æ—â–∞–¥—å –ø–∏–∫—Å–µ–ª—è
        dx = np.diff(xi)[0] if len(xi) > 1 else 1.0
        dy = np.diff(yi)[0] if len(yi) > 1 else 1.0
        area_per_pixel = dx * dy

        # –°—á–∏—Ç–∞–µ–º –ø–∏–∫—Å–µ–ª–∏ –≤ –∫–∞–∂–¥–æ–º –±–∞—Å—Å–µ–π–Ω–µ (0 ‚Äî —Ñ–æ–Ω, –∏—Å–∫–ª—é—á–∞–µ–º)
        unique_labels, counts = np.unique(labels_ws, return_counts=True)
        basin_areas = {
            int(label): float(count * area_per_pixel)
            for label, count in zip(unique_labels, counts)
            if label != 0  # Skip background
        }

        # –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º –º–∏–Ω–∏–º—É–º—É –µ–≥–æ –º–µ—Ç–∫—É –±–∞—Å—Å–µ–π–Ω–∞ –∏ –ø–ª–æ—â–∞–¥—å
        minima_labels = []
        for _, r in mins.iterrows():
            ix = int(np.searchsorted(xi, r['X']))
            iy = int(np.searchsorted(yi, r['Y']))
            ix = np.clip(ix, 0, ZI_work.shape[1] - 1)
            iy = np.clip(iy, 0, ZI_work.shape[0] - 1)
            label = int(labels_ws[iy, ix])
            minima_labels.append(label)

        mins = mins.copy()  # Ensure we don't modify original DataFrame outside scope
        mins['basin_label'] = minima_labels
        mins['basin_area_ws'] = mins['basin_label'].map(lambda lbl: basin_areas.get(lbl, 0.0))

    except Exception as e:
        st.warning(f"–°–µ–≥–º–µ–Ω—Ç–∞—Ü–∏—è –≤–æ–¥–æ—Ä–∞–∑–¥–µ–ª–æ–º –Ω–µ —É–¥–∞–ª–∞—Å—å: {e}")
        # Optionally log full traceback
        # import traceback; st.code(traceback.format_exc())
    else:
        # –ö–∞—Ä—Ç–∞ –±–∞—Å—Å–µ–π–Ω–æ–≤
        try:
            fig_ws = plot_watershed_matplotlib(XI, YI, labels_ws, minima_df=mins, title="–ö–∞—Ä—Ç–∞ –±–∞—Å—Å–µ–π–Ω–æ–≤ (–≤–æ–¥–æ—Ä–∞–∑–¥–µ–ª)")
            st.pyplot(fig_ws)
        except Exception:
            fig_ws = None

col1, col2 = st.columns([1, 1])
with col1:
    st.subheader("2D —Ç–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ + –∫–ª–∞—Å—Ç–µ—Ä—ã")

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¢–£ –ñ–ï –ø–∞–ª–∏—Ç—Ä—É, —á—Ç–æ –∏ –≤ 3D
    selected_cmap = st.session_state.get('surface_palette', 'twilight')

    fig2d = plot_heatmap_matplotlib(
        XI, YI, ZI,
        minima_df=mins,
        clusters='cluster',
        title="–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ + –∫–ª–∞—Å—Ç–µ—Ä—ã –º–∏–Ω–∏–º—É–º–æ–≤",
        cmap=selected_cmap  # <-- –ø–µ—Ä–µ–¥–∞—ë–º –≤—ã–±—Ä–∞–Ω–Ω—É—é –ø–∞–ª–∏—Ç—Ä—É
    )
    st.pyplot(fig2d)
with col2:
    st.subheader("3D –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç—å (–∏–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω–æ)")

    # –ì—Ä–∞—Ñ–∏–∫ 3D —Å—Ç—Ä–æ–∏—Ç—Å—è –≤—ã—à–µ
    fig3 = plot_3d_plotly(
        XI, YI, ZI,
        minima_df=mins,
        cluster_col='cluster',
        surface_colorscale=st.session_state.get('surface_palette', 'twilight'),
        cluster_palette=st.session_state.get('cluster_palette', 'Dark24')
    )
    st.plotly_chart(fig3, use_container_width=True)

    # === –≠–ª–µ–º–µ–Ω—Ç—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –ü–û–î –≥—Ä–∞—Ñ–∏–∫–æ–º ===
    with st.container():
        st.markdown("---")
        st.caption("üé® –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ü–≤–µ—Ç–æ–≤—ã—Ö —Å—Ö–µ–º:")

        # –í—ã–±–æ—Ä –ø–∞–ª–∏—Ç—Ä—ã –¥–ª—è surface (–≤–ª–∏—è–µ—Ç –Ω–∞ 2D –∏ 3D)
        surface_palette = st.selectbox(
            "–ü–∞–ª–∏—Ç—Ä–∞ –ø–æ–≤–µ—Ä—Ö–Ω–æ—Å—Ç–∏",
            options=[
                'viridis', 'plasma', 'inferno', 'cividis', 'twilight',
                'hot', 'jet', 'rainbow', 'coolwarm', 'terrain', 'ocean'
            ],
            format_func=str.title,
            index=4,  # default: 'twilight'
            key='surface_palette'
        )

        # –í—ã–±–æ—Ä –ø–∞–ª–∏—Ç—Ä—ã –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (—Ç–æ–ª—å–∫–æ –¥–ª—è —Ç–æ—á–µ–∫)
        cluster_palette = st.selectbox(
            "–ü–∞–ª–∏—Ç—Ä–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤",
            options=[
                'Dark24', 'Set1', 'Plotly', 'Bold', 'Safe', 'Vivid',
                'Pastel1', 'Paired', 'Accent', 'Dark2'
            ],
            index=0,
            key='cluster_palette'
        )

# –ü–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ df
# st.subheader("Exploratory Data Analysis (EDA)")
# run_eda(df)
# figq = plot_quiver(mins, scale=1.0, nmax=200)
# if figq:
#     st.subheader("Gradient vectors (sample)")
#     st.pyplot(figq)

#
# Outputs: CSV, PDF, ZIP
#
csv_bytes = mins.to_csv(index=False).encode('utf-8')
st.download_button("–°–∫–∞—á–∞—Ç—å CSV –º–∏–Ω–∏–º—É–º–æ–≤", data=csv_bytes, file_name="minima_with_features_clusters.csv", mime="text/csv")

imgs = []
# heatmap image
try:
    with io.BytesIO() as buf:
        fig2d.savefig(buf, format='png', bbox_inches='tight', dpi=150)
        buf.seek(0)
        img_data = buf.getvalue()  # —Å—á–∏—Ç—ã–≤–∞–µ–º –î–û –∑–∞–∫—Ä—ã—Ç–∏—è
        imgs.append(("heatmap.png", img_data))
    # buf –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∑–∞–∫—Ä—ã–≤–∞–µ—Ç—Å—è –ø—Ä–∏ –≤—ã—Ö–æ–¥–µ –∏–∑ `with`
except Exception as e:
    st.error(f"Error saving heatmap: {e}")


# 3d surface image
try:
    png3 = fig3.to_image(format="png", width=900, height=600, scale=1)
    imgs.append(("3d_surface.png", png3))
except Exception:
    pass

# watershed image (–µ—Å–ª–∏ –ø–æ—Å—Ç—Ä–æ–µ–Ω–∞ –∫–∞—Ä—Ç–∞ –±–∞—Å—Å–µ–π–Ω–æ–≤)
try:
    if do_watershed and 'fig_ws' in locals() and fig_ws is not None:
        with io.BytesIO() as buf:
            fig_ws.savefig(buf, format='png', bbox_inches='tight', dpi=150)
            buf.seek(0)
            imgs.append(("watershed.png", buf.getvalue()))
except Exception:
    pass

summary_table = mins.groupby('cluster')[['depth', 'mean_grad', 'basin_area_est']].agg(['count', 'mean', 'std']).reset_index()
params = {"R": R, "R_basin": R_basin, "min_neighbors": min_neighbors, "basin_frac": basin_frac, "clustering_method": method}
try:
    pdf_bytes = build_pdf_report("Terrain minima clustering", params, {"Cluster summary": summary_table}, imgs)
    st.download_button("Download PDF report", data=pdf_bytes, file_name="terrain_report.pdf", mime="application/pdf")
except Exception as e:
    st.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞—Ç—å PDF: {e}")

zipbuf = io.BytesIO()
with zipfile.ZipFile(zipbuf, "w") as zf:
    zf.writestr("minima.csv", csv_bytes)
    for fname, bts in imgs:
        zf.writestr(fname, bts)
    zf.writestr("params.json", json.dumps(params, indent=2))
zipbuf.seek(0)
st.download_button("–°–∫–∞—á–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (ZIP)", data=zipbuf.getvalue(), file_name="results.zip", mime="application/zip")

st.success("–ì–æ—Ç–æ–≤–æ. –ú–æ–∂–Ω–æ —Å–∫–∞—á–∞—Ç—å CSV / PDF / ZIP.")
